{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from scipy.linalg import block_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxopt import matrix, solvers\n",
    "solvers.options['show_progress'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soocer Environment\n",
    "class Soccer:\n",
    "    def __init__(self):\n",
    "        # initialization according to the figure 4 of the paper\n",
    "\n",
    "        # pos as list of 2 elements, 1st element as position of player A\n",
    "        # 2nd element as position of player B\n",
    "        self.pos = [np.array([0, 2]), np.array([0, 1])]\n",
    "        self.ball = 1\n",
    "        self.goal = [0, 3]\n",
    "\n",
    "\n",
    "    def move(self, actions):\n",
    "        # top-left corner as (0,0) origin point\n",
    "        # 0:North, 1:East, 2:South, 3:West, 4:Stick\n",
    "        # player can move at most one tile at a time\n",
    "        # the index of actions map to movement at specific direction\n",
    "        legal_actions = [[-1, 0], [0, 1], [1, 0], [0, -1], [0, 0]]\n",
    "\n",
    "        # players action executed in random order\n",
    "        # mover_first as the index 0 or 1\n",
    "        # index 0 is player A, index 1 is player B\n",
    "        mover_first = np.random.choice([0, 1], 1)[0]\n",
    "        mover_second = 1 - mover_first\n",
    "\n",
    "        # copy of current pos\n",
    "        new_pos = self.pos.copy()\n",
    "\n",
    "        # scores shows the reward for player A and player B\n",
    "        scores = np.array([0, 0])\n",
    "\n",
    "        # init termination status of the game\n",
    "        done = 0\n",
    "\n",
    "        # check whether the received action is valid\n",
    "        if actions[0] not in range(0,5) or actions[1] not in range(0,5):\n",
    "            print('Invalid Action, actions shall be in [0,1,2,3,4]')\n",
    "            return [self.pos[0][0] * 4 + self.pos[0][1], self.pos[1][0] * 4 + self.pos[1][1], self.ball], scores, done\n",
    "        else:\n",
    "            # moving the first player\n",
    "            new_pos[mover_first] = self.pos[mover_first] + legal_actions[actions[mover_first]]\n",
    "\n",
    "            # check collision, 1st mover collides with 2nd mover after moving\n",
    "            if (new_pos[mover_first] == self.pos[mover_second]).all():\n",
    "                # if 1st mover possess ball, the ball is lost to 2nd mover\n",
    "                if self.ball == mover_first:\n",
    "                    self.ball = mover_second\n",
    "\n",
    "            # no collision, update 1st mover's pos\n",
    "            elif new_pos[mover_first][0] in range(0,2) and new_pos[mover_first][1] in range(0,4):\n",
    "                self.pos[mover_first] = new_pos[mover_first]\n",
    "\n",
    "                # if scored for player himself\n",
    "                if self.pos[mover_first][1] == self.goal[mover_first] and self.ball == mover_first:      # Player scored\n",
    "                    scores = ([1, -1][mover_first]) * np.array([100, -100])\n",
    "                    done = 1\n",
    "                    return [self.pos[0][0] * 4 + self.pos[0][1], self.pos[1][0] * 4 + self.pos[1][1], self.ball], scores, done\n",
    "\n",
    "                # if scored for the opponent\n",
    "                elif self.pos[mover_first][1] == self.goal[mover_second] and self.ball == mover_first:\n",
    "                    scores = ([1, -1][mover_first]) * np.array([-100, 100])\n",
    "                    done = 1\n",
    "                    return [self.pos[0][0] * 4 + self.pos[0][1], self.pos[1][0] * 4 + self.pos[1][1], self.ball], scores, done\n",
    "\n",
    "\n",
    "            # moving the second player\n",
    "            new_pos[mover_second] = self.pos[mover_second] + legal_actions[actions[mover_second]]\n",
    "\n",
    "            # check collision, 2nd mover collides with 1st mover after moving\n",
    "            if (new_pos[mover_second] == self.pos[mover_first]).all():  # Collide\n",
    "                # if 2nd mover possess ball, the ball is lost to 1st mover\n",
    "                if self.ball == mover_second:\n",
    "                    self.ball = mover_first\n",
    "\n",
    "            # no collision, update 2nd mover's pos\n",
    "            elif new_pos[mover_second][0] in range(0,2) and new_pos[mover_second][1] in range(0,4):\n",
    "                self.pos[mover_second] = new_pos[mover_second]\n",
    "\n",
    "                # if scored for player himself\n",
    "                if self.pos[mover_second][1] == self.goal[mover_second] and self.ball == mover_second:\n",
    "                    scores = ([1, -1][mover_second]) * np.array([100, -100])\n",
    "                    done = 1\n",
    "                    return [self.pos[0][0] * 4 + self.pos[0][1], self.pos[1][0] * 4 + self.pos[1][1], self.ball], scores, done\n",
    "\n",
    "                # if scored for the opponent\n",
    "                elif self.pos[mover_second][1] == self.goal[mover_first] and self.ball == mover_second:\n",
    "                    scores = np.array([-100, 100]) * [1, -1][mover_second]\n",
    "                    done = 1\n",
    "                    return [self.pos[0][0] * 4 + self.pos[0][1], self.pos[1][0] * 4 + self.pos[1][1], self.ball], scores, done\n",
    "\n",
    "\n",
    "        return [self.pos[0][0] * 4 + self.pos[0][1], self.pos[1][0] * 4 + self.pos[1][1], self.ball], scores, done\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(no_steps = int(1e6)):\n",
    "\n",
    "    # Take action with epsilon-greedy r\n",
    "    def generate_action(Q, state, epsilon):\n",
    "        # epsilon-greey to take best action from action-value function\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.choice([0,1,2,3,4], 1)[0]\n",
    "\n",
    "        return np.random.choice(np.where(Q[state[0]][state[1]][state[2]] == max(Q[state[0]][state[1]][state[2]]))[0], 1)[0]\n",
    "\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # discount factor\n",
    "    gamma = 0.9\n",
    "\n",
    "    # Define the epsilon and its decay for epsilon-greedy action selection\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.001\n",
    "    epsilon_decay = 0.999995\n",
    "\n",
    "    # learning rate\n",
    "    alpha = 1.0\n",
    "    alpha_min = 0.001\n",
    "    alpha_decay = 0.999995\n",
    "    #end_alpha = 0.001\n",
    "\n",
    "    # store the step-error\n",
    "    error_list = []\n",
    "\n",
    "    # Q_tables of player A and player B\n",
    "    # the state-action space is 8 (pos for player A) * 8 (pos for player B) * 2 (ball possession) * 5 (valid actions)\n",
    "    Q_1 = np.zeros((8, 8, 2, 5))\n",
    "    Q_2 = np.zeros((8, 8, 2, 5))\n",
    "\n",
    "    # index for step\n",
    "    i = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    while i < no_steps:\n",
    "        env = Soccer()\n",
    "\n",
    "        # map two players positions and ball possession into state presentation\n",
    "        state = [env.pos[0][0] * 4 + env.pos[0][1], env.pos[1][0] * 4 + env.pos[1][1], env.ball]\n",
    "\n",
    "        while True:\n",
    "            if i % 1000 == 0:\n",
    "                print('\\rstep {}\\t Time: {:.2f} \\t Percentage: {:.2f}% \\t Alpha: {:.3f}'.format(i, time.time() - start_time, i*100/no_steps, alpha), end=\"\")\n",
    "\n",
    "            # player A at sate S take action South before update\n",
    "            # first index is player A's position index (0-7), 2 is first row (0), 3rd column\n",
    "            # second index is player B's position index (0-7), 1 is first row (0), 2nd column\n",
    "            # third index is ball possession, according to graph, B has the ball\n",
    "            # fourth index is action from player A, A is going south (2)\n",
    "            before = Q_1[2][1][1][2]\n",
    "\n",
    "            # eps-greedy to generate action\n",
    "            actions = [generate_action(Q_1,state,epsilon), generate_action(Q_2,state,epsilon)]\n",
    "            # get next state, reward and game termination flag\n",
    "            state_prime, rewards, done = env.move(actions)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "            # Q-learning for player A & B\n",
    "            if done:\n",
    "                Q_1[state[0]][state[1]][state[2]][actions[0]] = Q_1[state[0]][state[1]][state[2]][actions[0]] + alpha * (rewards[0] - Q_1[state[0]][state[1]][state[2]][actions[0]])\n",
    "\n",
    "                Q_2[state[0]][state[1]][state[2]][actions[1]] = Q_2[state[0]][state[1]][state[2]][actions[1]] + alpha * (rewards[1] - Q_2[state[0]][state[1]][state[2]][actions[1]])\n",
    "\n",
    "                # player A at state S take action South before update\n",
    "                after = Q_1[2][1][1][2]\n",
    "                error_list.append(abs(after-before))\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                Q_1[state[0]][state[1]][state[2]][actions[0]] = Q_1[state[0]][state[1]][state[2]][actions[0]] + alpha * (rewards[0] + gamma * max(Q_1[state_prime[0]][state_prime[1]][state_prime[2]]) - Q_1[state[0]][state[1]][state[2]][actions[0]])\n",
    "\n",
    "                Q_2[state[0]][state[1]][state[2]][actions[1]] = Q_2[state[0]][state[1]][state[2]][actions[1]] + alpha * (rewards[1] + gamma * max(Q_2[state_prime[0]][state_prime[1]][state_prime[2]]) - Q_2[state[0]][state[1]][state[2]][actions[1]])\n",
    "                state = state_prime\n",
    "\n",
    "                # player A at state S take action South before update\n",
    "                after = Q_1[2][1][1][2]\n",
    "                error_list.append(abs(after-before))\n",
    "\n",
    "            #decay epsilon and alpha\n",
    "            epsilon *= epsilon_decay\n",
    "            epsilon = max(epsilon_min, epsilon)\n",
    "\n",
    "            alpha *= alpha_decay\n",
    "            alpha = max(alpha_min, alpha)\n",
    "\n",
    "    return error_list, Q_1, Q_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning\n",
    "q_learning_errors, Q_1_qlearning, Q_2_qlearning = Q_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_plot(errors, title):\n",
    "    plt.plot(errors, linestyle='-', linewidth=0.6)\n",
    "    plt.title(title)\n",
    "    plt.ylim(0, 0.5)\n",
    "    plt.xlabel('Simulation Iteartion')\n",
    "    plt.ylabel('Q-value Difference')\n",
    "    plt.ticklabel_format(style='sci', axis='x',\n",
    "                         scilimits=(0,0), useMathText=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_plot(np.array(q_learning_errors), 'Q-learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Friend_Q(no_steps = int(1e6)):\n",
    "\n",
    "    # Take action with epsilon-greedy r\n",
    "    def generate_action(Q, state, epsilon):\n",
    "        # epsilon-greey to take best action from action-value function\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.choice([0,1,2,3,4], 1)[0]\n",
    "\n",
    "        max_idx = np.where(Q[state[0]][state[1]][state[2]] == np.max(Q[state[0]][state[1]][state[2]]))\n",
    "        return max_idx[1][np.random.choice(range(len(max_idx[0])), 1)[0]]\n",
    "\n",
    "    np.random.seed(1234)\n",
    "\n",
    "    # discount factor\n",
    "    gamma = 0.9\n",
    "\n",
    "    # Define the epsilon and its decay for epsilon-greedy action selection\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.001\n",
    "    epsilon_decay = 0.999995\n",
    "\n",
    "    # epsilon_begin = 0.1\n",
    "    # epsilon_end = 0\n",
    "    # epsilon_periods = no_steps/2\n",
    "\n",
    "\n",
    "    # learning rate\n",
    "    alpha = 1.0\n",
    "    alpha_min = 0.001\n",
    "    alpha_decay = 0.999995\n",
    "    #end_alpha = 0.001\n",
    "\n",
    "    # store the step-error\n",
    "    error_list = []\n",
    "\n",
    "    # Q_tables of player A and player B\n",
    "    # the state-action space is 8 (pos for player A) * 8 (pos for player B) * 2 (ball possession) * 5 (valid actions for player A) * 5 (valid actions for player B)\n",
    "    Q_1 = np.zeros((8, 8, 2, 5, 5))\n",
    "    Q_2 = np.zeros((8, 8, 2, 5, 5))\n",
    "\n",
    "    # index for step\n",
    "    i = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    while i < no_steps:\n",
    "        env = Soccer()\n",
    "\n",
    "        # map two players positions and ball possession into state presentation\n",
    "        state = [env.pos[0][0] * 4 + env.pos[0][1], env.pos[1][0] * 4 + env.pos[1][1], env.ball]\n",
    "\n",
    "        while True:\n",
    "            # if (i + 1) % 1000 == 0: print('\\r', i + 1, ', Time lapse', time.time() - start_time, 'seconds for ', i*100/no_steps, \"%\", end = \"\")\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                print('\\rstep {}\\t Time: {:.2f} \\t Percentage: {:.2f}% \\t Alpha: {:.3f}'.format(i, time.time() - start_time, i*100/no_steps, alpha), end=\"\")\n",
    "\n",
    "            # player A at sate S take action South before update\n",
    "            # first index is player A's position index (0-7), 2 is frist row (0), 3rd column\n",
    "            # second index is player B's position index (0-7), 1 is first row (0), 2nd column\n",
    "            # third index is ball possession, according to graph, B has the ball\n",
    "            # fourth index is action from player B, B sticks\n",
    "            # fifth index is action from player A, A goes south\n",
    "            # rationale for putting player A's action as last index is for easy handling of max function (put the last dimention as player's action rather than opponent's action)\n",
    "            before = Q_1[2][1][1][4][2]\n",
    "\n",
    "            # eps-greedy to generate action\n",
    "            actions = [generate_action(Q_1,state,epsilon), generate_action(Q_2,state,epsilon)]\n",
    "            # get next state, reward and game termination flag\n",
    "            state_prime, rewards, done = env.move(actions)\n",
    "\n",
    "            alpha = 1 / (i / alpha_min / no_steps + 1)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "            # Friend-Q is Q-learning adding dimension of oppenent's action\n",
    "            if done:\n",
    "                Q_1[state[0]][state[1]][state[2]][actions[1]][actions[0]] = Q_1[state[0]][state[1]][state[2]][actions[1]][actions[0]] + alpha * (rewards[0] - Q_1[state[0]][state[1]][state[2]][actions[1]][actions[0]])\n",
    "\n",
    "                Q_2[state[0]][state[1]][state[2]][actions[0]][actions[1]] = Q_2[state[0]][state[1]][state[2]][actions[0]][actions[1]] + alpha * (rewards[1] - Q_2[state[0]][state[1]][state[2]][actions[0]][actions[1]])\n",
    "\n",
    "                # player A at state S take action South before update\n",
    "                after = Q_1[2][1][1][4][2]\n",
    "                error_list.append(abs(after-before))\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                Q_1[state[0]][state[1]][state[2]][actions[1]][actions[0]] = Q_1[state[0]][state[1]][state[2]][actions[1]][actions[0]] + alpha * (rewards[0] + gamma * np.max(Q_1[state_prime[0]][state_prime[1]][state_prime[2]]) - Q_1[state[0]][state[1]][state[2]][actions[1]][actions[0]])\n",
    "\n",
    "                Q_2[state[0]][state[1]][state[2]][actions[0]][actions[1]] = Q_2[state[0]][state[1]][state[2]][actions[0]][actions[1]] + alpha * (rewards[1] + gamma * np.max(Q_2[state_prime[0]][state_prime[1]][state_prime[2]]) - Q_2[state[0]][state[1]][state[2]][actions[0]][actions[1]])\n",
    "                state = state_prime\n",
    "\n",
    "                # player A at state S take action South before update\n",
    "                after = Q_1[2][1][1][4][2]\n",
    "                error_list.append(abs(after-before))\n",
    "\n",
    "            #decay epsilon and alpha\n",
    "            # alpha *= alpha_decay\n",
    "            # alpha = max(alpha_min, alpha)\n",
    "\n",
    "            epsilon *= epsilon_decay\n",
    "            epsilon = max(epsilon_min, epsilon)\n",
    "\n",
    "    return error_list, Q_1, Q_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Friend-Q\n",
    "friend_q_errors, Q_1_friend, Q_2_friend = Friend_Q()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_plot(np.array(friend_q_errors)[np.where(np.array(friend_q_errors) > 0)], 'Friend-Q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foe-Q Learning\n",
    "def Foe_Q(no_steps = int(1e6)):\n",
    "\n",
    "    # Take action with epsilon-greedy\n",
    "    def generate_action(pi, state, i):\n",
    "        # epsilon-greey to take best action from action-value function\n",
    "        # decay epsilon\n",
    "        epsilon = epsilon_decay ** i\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.choice([0,1,2,3,4], 1)[0]\n",
    "        else:\n",
    "            return np.random.choice([0,1,2,3,4], 1, p=pi[state[0]][state[1]][state[2]])[0]\n",
    "\n",
    "    # same formulation as hw6\n",
    "    # Q value is just like the reward matrix\n",
    "    def max_min(Q, state):\n",
    "        c = matrix([-1.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "        G = matrix(np.append(np.append(np.ones((5,1)), -Q[state[0]][state[1]][state[2]], axis=1), np.append(np.zeros((5,1)), -np.eye(5), axis=1), axis=0))\n",
    "        h = matrix([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "        A = matrix([[0.0],[1.0], [1.0], [1.0], [1.0], [1.0]])\n",
    "        b = matrix(1.0)\n",
    "        sol = solvers.lp(c=c, G=G, h=h, A=A, b=b)\n",
    "        return np.abs(sol['x'][1:]).reshape((5,)) / sum(np.abs(sol['x'][1:])), np.array(sol['x'][0])\n",
    "\n",
    "    # discount factor\n",
    "    gamma = 0.9\n",
    "\n",
    "    # Define the epsilon and its decay for epsilon-greedy action selection\n",
    "    epsilon_min = 0.001\n",
    "    epsilon_decay = 10**(np.log10(epsilon_min)/no_steps)\n",
    "    # epsilon_min = 0.001\n",
    "    # epsilon_decay = 0.999995\n",
    "\n",
    "    # learning rate\n",
    "    alpha = 1.0 \n",
    "    alpha_min = 0.001\n",
    "    alpha_decay = 10**(np.log10(alpha_min)/no_steps)\n",
    "\n",
    "\n",
    "    # Q_tables of player A and player B\n",
    "    # the state-action space is 8 (pos for player A) * 8 (pos for player B) * 2 (ball possession) * 5 (valid actions for player A) * 5 (valid actions for player B)\n",
    "    # initialization to 1 in order to break from zero\n",
    "    Q_1 = np.ones((8, 8, 2, 5, 5)) * 1.0\n",
    "    Q_2 = np.ones((8, 8, 2, 5, 5)) * 1.0\n",
    "\n",
    "    # init policy for player 1 and player 2\n",
    "    Pi_1 = np.ones((8, 8, 2, 5)) * 1/5\n",
    "    Pi_2 = np.ones((8, 8, 2, 5)) * 1/5\n",
    "\n",
    "    # value of states, only depends on pos of players and possession of ball\n",
    "    V_1 = np.ones((8, 8, 2)) * 1.0\n",
    "    V_2 = np.ones((8, 8, 2)) * 1.0\n",
    "\n",
    "    # error list to store ERR\n",
    "    errors_list = []\n",
    "\n",
    "    # set seed\n",
    "    np.random.seed(1234)\n",
    "\n",
    "    # Loop for no_steps steps\n",
    "    start_time = time.time()\n",
    "    i = 0\n",
    "\n",
    "    while i < no_steps:\n",
    "        soccer = Soccer()\n",
    "        state = [soccer.pos[0][0] * 4 + soccer.pos[0][1], soccer.pos[1][0] * 4 + soccer.pos[1][1], soccer.ball]\n",
    "        done = 0\n",
    "        while not done:\n",
    "            if i % 1000 == 0:\n",
    "                print('\\rstep {}\\t Time: {:.2f} \\t Percentage: {:.2f}% \\t Alpha: {:.3f}'.format(i, time.time() - start_time, i*100/no_steps, alpha), end=\"\")\n",
    "            i += 1\n",
    "\n",
    "            # player A at sate S take action South before update\n",
    "            # first index is player A's position index (0-7), 2 is frist row (0), 3rd column\n",
    "            # second index is player B's position index (0-7), 1 is first row (0), 2nd column\n",
    "            # third index is ball possession, according to graph, B has the ball\n",
    "            # fourth index is action from player B, B sticks\n",
    "            # fifth index is action from player A, A goes south\n",
    "            # rationale for putting player A's action as last index is for easy handling of max function (put the last dimention as player's action rather than opponent's action)\n",
    "            before = Q_1[2][1][1][4][2]\n",
    "\n",
    "            # eps-greedy to generate action\n",
    "            actions = [generate_action(Pi_1, state, i), generate_action(Pi_2, state, i)]\n",
    "\n",
    "            # get next state, reward and game termination flag\n",
    "            state_prime, rewards, done = soccer.move(actions)\n",
    "\n",
    "            # Q-learning update\n",
    "            Q_1[state[0]][state[1]][state[2]][actions[1]][actions[0]] = (1 - alpha) * Q_1[state[0]][state[1]][state[2]][actions[1]][actions[0]] + alpha * (rewards[0] + gamma * V_1[state_prime[0]][state_prime[1]][state_prime[2]])\n",
    "\n",
    "            # use LP to solve maxmin\n",
    "            pi, val = max_min(Q_1, state)\n",
    "            Pi_1[state[0]][state[1]][state[2]] = pi\n",
    "            V_1[state[0]][state[1]][state[2]] = val\n",
    "\n",
    "            # Q-learning update\n",
    "            Q_2[state[0]][state[1]][state[2]][actions[0]][actions[1]] = (1 - alpha) * Q_2[state[0]][state[1]][state[2]][actions[0]][actions[1]] + alpha * (rewards[1] + gamma * V_2[state_prime[0]][state_prime[1]][state_prime[2]])\n",
    "\n",
    "            # use LP to solve maxmin\n",
    "            pi, val = max_min(Q_2, state)\n",
    "            Pi_2[state[0]][state[1]][state[2]] = pi\n",
    "            V_2[state[0]][state[1]][state[2]] = val\n",
    "            state = state_prime\n",
    "\n",
    "            # compute ERR\n",
    "            after = Q_1[2][1][1][4][2]\n",
    "            errors_list.append(np.abs(after - before))\n",
    "\n",
    "            # decay learning rate\n",
    "            alpha = alpha_decay ** i\n",
    "\n",
    "    return errors_list, Q_1, Q_2, V_1, V_2, Pi_1, Pi_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Foe-Q\n",
    "foe_q_errors, Q_1_foe, Q_2_foe, V_1_foe, V_2_foe, Pi_1_foe, Pi_2_foe = Foe_Q()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_plot(np.array(foe_q_errors), 'Foe-Q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.array(foe_q_errors)[np.where(np.array(foe_q_errors) > 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CE_Q(no_steps = int(1e6)):\n",
    "\n",
    "    # Take action with epsilon-greedy\n",
    "    def take_action(Pi, state, i):\n",
    "        # epsilon-greey to take best action from action-value function\n",
    "        # decay epsilon\n",
    "        epsilon = epsilon_decay ** i\n",
    "\n",
    "        if np.random.random() < epsilon:\n",
    "            index = np.random.choice(np.arange(25), 1)\n",
    "            return np.array([index // 5, index % 5]).reshape(2)\n",
    "\n",
    "        else:\n",
    "            index = np.random.choice(np.arange(25), 1, p=Pi[state[0]][state[1]][state[2]].reshape(25))\n",
    "            return np.array([index // 5, index % 5]).reshape(2)\n",
    "\n",
    "    # using LP to solve correlated-equilibrium\n",
    "    def solve_ce(Q_1, Q_2, state):\n",
    "        # subset the condition for player A\n",
    "        Q_states = Q_1[state[0]][state[1]][state[2]]\n",
    "        s = block_diag(Q_states - Q_states[0, :], Q_states - Q_states[1, :], Q_states - Q_states[2, :], Q_states - Q_states[3, :], Q_states - Q_states[4, :])\n",
    "        row_index = [1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23]\n",
    "        parameters_1 = s[row_index, :]\n",
    "\n",
    "        # subset the condition for player B\n",
    "        Q_states = Q_2[state[0]][state[1]][state[2]]\n",
    "        s = block_diag(Q_states - Q_states[0, :], Q_states - Q_states[1, :], Q_states - Q_states[2, :], Q_states - Q_states[3, :], Q_states - Q_states[4, :])\n",
    "        col_index = [0, 5, 10, 15, 20, 1, 6, 11, 16, 21, 2, 7, 12, 17, 22, 3, 8, 13, 18, 23, 4, 9, 14, 19, 24]\n",
    "        parameters_2 = s[row_index, :][:, col_index]\n",
    "\n",
    "        c = matrix((Q_1[state[0]][state[1]][state[2]] + Q_2[state[0]][state[1]][state[2]].T).reshape(25))\n",
    "        # construct rationality constraints\n",
    "        G = matrix(np.append(np.append(parameters_1, parameters_2, axis=0), -np.eye(25), axis=0))\n",
    "        h = matrix(np.zeros(65) * 0.0)\n",
    "        # construct probability constraints\n",
    "        A = matrix(np.ones((1, 25)))\n",
    "        b = matrix(1.0)\n",
    "\n",
    "        # error-handling mechanism\n",
    "        try:\n",
    "            sol = solvers.lp(c=c, G=G, h=h, A=A, b=b)\n",
    "            if sol['x'] is not None:\n",
    "                prob = np.abs(np.array(sol['x']).reshape((5, 5))) / sum(np.abs(sol['x']))\n",
    "                val_1 = np.sum(prob * Q_1[state[0]][state[1]][state[2]])\n",
    "                val_2 = np.sum(prob * Q_2[state[0]][state[1]][state[2]].T)\n",
    "            else:\n",
    "                prob = None\n",
    "                val_1 = None\n",
    "                val_2 = None\n",
    "        except:\n",
    "            print(\"error!!\")\n",
    "            prob = None\n",
    "            val_1 = None\n",
    "            val_2 = None\n",
    "\n",
    "        return prob, val_1, val_2\n",
    "\n",
    "\n",
    "    # discount rate\n",
    "    gamma = 0.9\n",
    "    epsilon_min = 0.001\n",
    "    epsilon_decay = 10 ** (np.log10(epsilon_min)/no_steps)\n",
    "    # epsilon_min = 0.001\n",
    "    # epsilon_decay = 0.999995\n",
    "\n",
    "    # learning rate\n",
    "    alpha_min = 0.001\n",
    "    alpha_decay = 10 ** (np.log10(alpha_min)/no_steps)\n",
    "\n",
    "    # Q_tables of player A and player B\n",
    "    # the state-action space is 8 (pos for player A) * 8 (pos for player B) * 2 (ball possession) * 5 (valid actions for player A) * 5 (valid actions for player B)\n",
    "    Q_1 = np.ones((8, 8, 2, 5, 5)) * 1.0\n",
    "    Q_2 = np.ones((8, 8, 2, 5, 5)) * 1.0\n",
    "\n",
    "    # value of states, only depends on pos of players and possession of ball\n",
    "    V_1 = np.ones((8, 8, 2)) * 1.0\n",
    "    V_2 = np.ones((8, 8, 2)) * 1.0\n",
    "\n",
    "    # shared joint policy\n",
    "    Pi = np.ones((8, 8, 2, 5, 5)) * 1/25\n",
    "\n",
    "    # error list to store ERR\n",
    "    error_list = []\n",
    "\n",
    "    # set seed\n",
    "    np.random.seed(1234)\n",
    "\n",
    "    start_time = time.time()\n",
    "    i = 0\n",
    "    while i < no_steps:\n",
    "        soccer = Soccer()\n",
    "        state = [soccer.pos[0][0] * 4 + soccer.pos[0][1], soccer.pos[1][0] * 4 + soccer.pos[1][1], soccer.ball]\n",
    "        done = 0\n",
    "        j = 0\n",
    "        while not done and j <= 100:\n",
    "            if i % 1000 == 0:\n",
    "                print('\\rstep {}\\t Time: {:.2f} \\t Percentage: {:.2f}% \\t Alpha: {:.3f}'.format(i, time.time() - start_time, i*100/no_steps, alpha), end=\"\")\n",
    "\n",
    "            # udpate index\n",
    "            i, j = i+1, j+1\n",
    "\n",
    "            # we don't need place player B action space before player A\n",
    "            # since we are no longer just selecting the max of player A\n",
    "            before = Q_1[2][1][1][2][4]\n",
    "\n",
    "            # eps-greedy to generate action\n",
    "            actions = take_action(Pi, state, i)\n",
    "\n",
    "            state_prime, rewards, done = soccer.move(actions)\n",
    "            alpha = alpha_decay ** i\n",
    "\n",
    "            # Q-learning update\n",
    "            Q_1[state[0]][state[1]][state[2]][actions[0]][actions[1]] = (1 - alpha) * Q_1[state[0]][state[1]][state[2]][actions[0]][actions[1]] + alpha * (rewards[0] + gamma * V_1[state_prime[0]][state_prime[1]][state_prime[2]])\n",
    "\n",
    "            # Q-learning update\n",
    "            Q_2[state[0]][state[1]][state[2]][actions[1]][actions[0]] = (1 - alpha) * Q_2[state[0]][state[1]][state[2]][actions[1]][actions[0]] + alpha * (rewards[1] + gamma * V_2[state_prime[0]][state_prime[1]][state_prime[2]].T)\n",
    "            prob, val_1, val_2 = solve_ce(Q_1, Q_2, state)\n",
    "\n",
    "            # update only if not Null returned from the ce solver\n",
    "            if prob is not None:\n",
    "                Pi[state[0]][state[1]][state[2]] = prob\n",
    "                V_1[state[0]][state[1]][state[2]] = val_1\n",
    "                V_2[state[0]][state[1]][state[2]] = val_2\n",
    "            state = state_prime\n",
    "\n",
    "            # player A at state S take action South after update\n",
    "            after = Q_1[2][1][1][2][4]\n",
    "            # compute the error\n",
    "            error_list.append(np.abs(after - before))\n",
    "\n",
    "    return error_list, Q_1, Q_2, V_1, V_2, Pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CE_Q\n",
    "ce_q_errors, Q_1_ce, Q_2_ce, V_1_ce, V_2_ce, Pi_ce = CE_Q()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_plot(np.array(ce_q_errors), 'CE-Q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
