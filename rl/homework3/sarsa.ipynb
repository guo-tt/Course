{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA Frozen Lake\n",
    "Implementation of a SARSA agent to learn policies in the Frozen Lake environment from OpenAI gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependency\n",
    "import gym # OpenAI Game Environment\n",
    "import gym.envs.toy_text # Customized Map\n",
    "import numpy as np\n",
    "from tqdm import trange # Processing Bar\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem - Frozen Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Frozen Lake](https://gym.openai.com/envs/FrozenLake-v0/) is an environment where an agent is able to move a character in a grid world. Starting from the state *S*, the agent aims to move the character to the goal state *G* for a reward of 1. Although the agent can pick one of four possible actions at each state including *left*, *down*, *right*, *up*, it only succeeds $\\frac{1}{3}$ of the times due to the slippery frozen state *F*. The agent is likely to move to any other directions for the remaining $\\frac{2}{3}$ times evenly. Additionally, stepping in a hole state *H* will lead to a bad ending with a reward of 0.\n",
    "\n",
    "+ S: Start State\n",
    "+ G: Goal State\n",
    "+ F: Frozen Surface\n",
    "+ H: Hole State\n",
    "\n",
    "![Frozen Lake](img/frozen_lake_0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 states\n",
      "4 actions\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "HFFF\n",
      "FFFF\n",
      "FFFG\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "amap='SFFFHFFFFFFFFFFG'\n",
    "grid_shape = np.int(np.sqrt(len(amap)))\n",
    "custom_map = np.array(list(amap)).reshape(grid_shape, grid_shape)\n",
    "env = gym.envs.toy_text.frozen_lake.FrozenLakeEnv(desc=custom_map, is_slippery=True).unwrapped\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "n_states, n_actions = env.observation_space.n, env.action_space.n\n",
    "print('{} states'.format(n_states))\n",
    "print('{} actions'.format(n_actions))\n",
    "env.render()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mH\u001b[0mFFF\n",
      "FFFF\n",
      "FFFG\n"
     ]
    }
   ],
   "source": [
    "# take a look\n",
    "done = False\n",
    "env.reset()\n",
    "while not done:\n",
    "    # randomly pick an action\n",
    "    action = np.random.randint(n_actions)\n",
    "    # get feedback from the environment\n",
    "    obvervation, reward, done, info = env.step(action)\n",
    "    # show the environment\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Action Reward State Action (SARSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[State Action Reward State Action (SARSA)](https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action) is a classic method learning a Markov Decision Process (MDP) policy to solve problems in the field of reinforcement learning. As indicated by the name SARSA, it updates the $Q(s_{t}, a_{t})$, according to the current state $s_{t}$, the action choose $a_{t}$, the reward $r_{t}$ due to this action, the new state $s_{t+1}$ after taking this action, and the action $a_{t+1}$ picked for this new state.\n",
    "Given that, the Q-value table can be updated by:\n",
    "\n",
    "$$Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha[r_{t} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_{t}, a_{t})]$$\n",
    "\n",
    "where, $\\alpha$ stands for the learning rate and $\\gamma$ represents the discount factor. It can be seen in the definition that the SARSA method aims to update the policy through interactions with the environment, so it belongs to the on-policy learning algorithm family."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the agent’s Q-table to zeros\n",
    "def init_q(s, a):\n",
    "    \"\"\"\n",
    "    s: number of states\n",
    "    a: number of actions\n",
    "    \"\"\"\n",
    "    return np.zeros((s, a))\n",
    "\n",
    "# epsilon-greedy exploration strategy\n",
    "def epsilon_greedy(Q, epsilon, n_actions, s):\n",
    "    \"\"\"\n",
    "    Q: Q Table\n",
    "    epsilon: exploration parameter\n",
    "    n_actions: number of actions\n",
    "    s: state\n",
    "    \"\"\"\n",
    "    # selects a random action with probability epsilon\n",
    "    if np.random.random() <= epsilon:\n",
    "        return np.random.randint(n_actions)\n",
    "    else:\n",
    "        return np.argmax(Q[s, :])\n",
    "    \n",
    "# SARSA Process\n",
    "def sarsa(alpha, gamma, epsilon, n_episodes):\n",
    "    \"\"\"\n",
    "    alpha: learning rate\n",
    "    gamma: exploration parameter\n",
    "    n_episodes: number of episodes\n",
    "    \"\"\"\n",
    "    # initialize Q table\n",
    "    Q = init_q(n_states, n_actions)\n",
    "    t = trange(n_episodes)\n",
    "    reward_array = np.zeros(n_episodes)\n",
    "    for i in t:\n",
    "        # initial state\n",
    "        s = env.reset()\n",
    "        # initial action\n",
    "        a = epsilon_greedy(Q, epsilon, n_actions, s)\n",
    "        done = False\n",
    "        while not done:\n",
    "            s_, reward, done, _ = env.step(a)\n",
    "            a_ = epsilon_greedy(Q, epsilon, n_actions, s_)\n",
    "            # update Q table\n",
    "            Q[s, a] += alpha * (reward + (gamma * Q[s_, a_]) - Q[s, a])\n",
    "            if done:\n",
    "#                 t.set_description('Episode {} Reward {}'.format(i + 1, reward))\n",
    "#                 t.refresh()\n",
    "                reward_array[i] = reward\n",
    "                break\n",
    "            s, a = s_, a_\n",
    "    env.close()\n",
    "    return Q, reward_array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment settings\n",
    "alpha = 0.25 # learning rate\n",
    "gamma = 1.0 # discount factor\n",
    "epsilon=0.29 # exploration parameter\n",
    "n_episodes = 14697  # number of training episodes\n",
    "np.random.seed(741684) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 14697 Reward 1.0: 100%|██████████| 14697/14697 [01:06<00:00, 220.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "Q, reward = sarsa(alpha, gamma, epsilon, n_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36056764, 0.54765592, 0.53477343, 0.75206226],\n",
       "       [0.68993437, 0.6489726 , 0.72299097, 0.82219617],\n",
       "       [0.869973  , 0.88669754, 0.93351916, 0.82829123],\n",
       "       [0.93381796, 0.93253269, 0.94729486, 0.91578574],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.26542696, 0.22786288, 0.85165137, 0.73450378],\n",
       "       [0.87363453, 0.89477981, 0.92076252, 0.89000001],\n",
       "       [0.93498558, 0.96450615, 0.95163416, 0.91824267],\n",
       "       [0.61137549, 0.84120554, 0.4918512 , 0.62596831],\n",
       "       [0.66558452, 0.89029202, 0.82599748, 0.74539858],\n",
       "       [0.88804912, 0.94793042, 0.91143964, 0.91659273],\n",
       "       [0.95812485, 0.98552116, 0.96425939, 0.95837349],\n",
       "       [0.85681271, 0.86301515, 0.85280369, 0.8206385 ],\n",
       "       [0.88022332, 0.86549787, 0.9247272 , 0.77877382],\n",
       "       [0.93910151, 0.97785198, 0.94577155, 0.93788063],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show Q table\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 3, 3, 2, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trained SARSA agent in Frozen Lake\n",
    "done = False\n",
    "s = env.reset()\n",
    "# env.render()\n",
    "actions = []\n",
    "while not done:\n",
    "    # pick an action\n",
    "    a = np.argmax(Q[s])\n",
    "    actions.append(a) \n",
    "    # get feedback from the environment\n",
    "    s_, _, done, _ = env.step(a)\n",
    "    # show the environment\n",
    "#     env.render()\n",
    "    s = s_\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
